{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02910d3c",
   "metadata": {},
   "source": [
    "# Extracting the text data from given URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a20707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\n",
      "Article saved for URL_ID: blackassign0001 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0001.txt\n",
      "Article saved for URL_ID: blackassign0002 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0002.txt\n",
      "Article saved for URL_ID: blackassign0003 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0003.txt\n",
      "Article saved for URL_ID: blackassign0004 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0004.txt\n",
      "Article saved for URL_ID: blackassign0005 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0005.txt\n",
      "Article saved for URL_ID: blackassign0006 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0006.txt\n",
      "Article saved for URL_ID: blackassign0007 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0007.txt\n",
      "Article saved for URL_ID: blackassign0008 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0008.txt\n",
      "Article saved for URL_ID: blackassign0009 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0009.txt\n",
      "Article saved for URL_ID: blackassign0010 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0010.txt\n",
      "Article saved for URL_ID: blackassign0011 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0011.txt\n",
      "Article saved for URL_ID: blackassign0012 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0012.txt\n",
      "Article saved for URL_ID: blackassign0013 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0013.txt\n",
      "Article saved for URL_ID: blackassign0014 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0014.txt\n",
      "Article saved for URL_ID: blackassign0015 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0015.txt\n",
      "Article saved for URL_ID: blackassign0016 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0016.txt\n",
      "Article saved for URL_ID: blackassign0017 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0017.txt\n",
      "Article saved for URL_ID: blackassign0018 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0018.txt\n",
      "Article saved for URL_ID: blackassign0019 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0019.txt\n",
      "Article saved for URL_ID: blackassign0020 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0020.txt\n",
      "Article saved for URL_ID: blackassign0021 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0021.txt\n",
      "Article saved for URL_ID: blackassign0022 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0022.txt\n",
      "Article saved for URL_ID: blackassign0023 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0023.txt\n",
      "Article saved for URL_ID: blackassign0024 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0024.txt\n",
      "Article saved for URL_ID: blackassign0025 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0025.txt\n",
      "Article saved for URL_ID: blackassign0026 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0026.txt\n",
      "Article saved for URL_ID: blackassign0027 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0027.txt\n",
      "Article saved for URL_ID: blackassign0028 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0028.txt\n",
      "Article saved for URL_ID: blackassign0029 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0029.txt\n",
      "Article saved for URL_ID: blackassign0030 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0030.txt\n",
      "Article saved for URL_ID: blackassign0031 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0031.txt\n",
      "Article saved for URL_ID: blackassign0032 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0032.txt\n",
      "Article saved for URL_ID: blackassign0033 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0033.txt\n",
      "Article saved for URL_ID: blackassign0034 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0034.txt\n",
      "Article saved for URL_ID: blackassign0035 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0035.txt\n",
      "Article saved for URL_ID: blackassign0037 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0037.txt\n",
      "Article saved for URL_ID: blackassign0038 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0038.txt\n",
      "Article saved for URL_ID: blackassign0039 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0039.txt\n",
      "Article saved for URL_ID: blackassign0040 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0040.txt\n",
      "Article saved for URL_ID: blackassign0041 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0041.txt\n",
      "Article saved for URL_ID: blackassign0042 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0042.txt\n",
      "Article saved for URL_ID: blackassign0043 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0043.txt\n",
      "Article saved for URL_ID: blackassign0044 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0044.txt\n",
      "Article saved for URL_ID: blackassign0045 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0045.txt\n",
      "Article saved for URL_ID: blackassign0046 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0046.txt\n",
      "Article saved for URL_ID: blackassign0047 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0047.txt\n",
      "Article saved for URL_ID: blackassign0048 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0048.txt\n",
      "Article saved for URL_ID: blackassign0050 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0050.txt\n",
      "Article saved for URL_ID: blackassign0051 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0051.txt\n",
      "Article saved for URL_ID: blackassign0052 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0052.txt\n",
      "Article saved for URL_ID: blackassign0053 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0053.txt\n",
      "Article saved for URL_ID: blackassign0054 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0054.txt\n",
      "Article saved for URL_ID: blackassign0055 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0055.txt\n",
      "Article saved for URL_ID: blackassign0056 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0056.txt\n",
      "Article saved for URL_ID: blackassign0057 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0057.txt\n",
      "Article saved for URL_ID: blackassign0058 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0058.txt\n",
      "Article saved for URL_ID: blackassign0059 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0059.txt\n",
      "Article saved for URL_ID: blackassign0060 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0060.txt\n",
      "Article saved for URL_ID: blackassign0061 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0061.txt\n",
      "Article saved for URL_ID: blackassign0062 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0062.txt\n",
      "Article saved for URL_ID: blackassign0063 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0063.txt\n",
      "Article saved for URL_ID: blackassign0064 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0064.txt\n",
      "Article saved for URL_ID: blackassign0065 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0065.txt\n",
      "Article saved for URL_ID: blackassign0066 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0066.txt\n",
      "Article saved for URL_ID: blackassign0067 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0067.txt\n",
      "Article saved for URL_ID: blackassign0068 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0068.txt\n",
      "Article saved for URL_ID: blackassign0069 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0069.txt\n",
      "Article saved for URL_ID: blackassign0070 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0070.txt\n",
      "Article saved for URL_ID: blackassign0071 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0071.txt\n",
      "Article saved for URL_ID: blackassign0072 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0072.txt\n",
      "Article saved for URL_ID: blackassign0073 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0073.txt\n",
      "Article saved for URL_ID: blackassign0074 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0074.txt\n",
      "Article saved for URL_ID: blackassign0075 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0075.txt\n",
      "Article saved for URL_ID: blackassign0076 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0076.txt\n",
      "Article saved for URL_ID: blackassign0077 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0077.txt\n",
      "Article saved for URL_ID: blackassign0078 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0078.txt\n",
      "Article saved for URL_ID: blackassign0079 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0079.txt\n",
      "Article saved for URL_ID: blackassign0080 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0080.txt\n",
      "Article saved for URL_ID: blackassign0081 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0081.txt\n",
      "Article saved for URL_ID: blackassign0082 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0082.txt\n",
      "Article saved for URL_ID: blackassign0083 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0083.txt\n",
      "Article saved for URL_ID: blackassign0084 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0084.txt\n",
      "Article saved for URL_ID: blackassign0085 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0085.txt\n",
      "Article saved for URL_ID: blackassign0086 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0086.txt\n",
      "Article saved for URL_ID: blackassign0087 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0087.txt\n",
      "Article saved for URL_ID: blackassign0088 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0088.txt\n",
      "Article saved for URL_ID: blackassign0089 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0089.txt\n",
      "Article saved for URL_ID: blackassign0090 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0090.txt\n",
      "Article saved for URL_ID: blackassign0091 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0091.txt\n",
      "Article saved for URL_ID: blackassign0092 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0092.txt\n",
      "Article saved for URL_ID: blackassign0093 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0093.txt\n",
      "Article saved for URL_ID: blackassign0094 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0094.txt\n",
      "Article saved for URL_ID: blackassign0095 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0095.txt\n",
      "Article saved for URL_ID: blackassign0096 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0096.txt\n",
      "Article saved for URL_ID: blackassign0097 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0097.txt\n",
      "Article saved for URL_ID: blackassign0098 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0098.txt\n",
      "Article saved for URL_ID: blackassign0099 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0099.txt\n",
      "Article saved for URL_ID: blackassign0100 C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\\blackassign0100.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Read the input Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\Input.xlsx')\n",
    "\n",
    "# current_dir = os.getcwd()\n",
    "# print(\"Current working directory:\", current_dir)\n",
    "output_dir=r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results'\n",
    "\n",
    "# # Specify the new directory path\n",
    "new_directory = (r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results')\n",
    "\n",
    "# # Change the current working directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Verify the new current working directory\n",
    "current_dir = os.getcwd()\n",
    "                 \n",
    "print(\"Current working directory:\", current_dir)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the article title and text based on the HTML structure of the webpage\n",
    "    article_title = soup.find('h1')\n",
    "    if article_title is None:\n",
    "        article_title = soup.find('title')  # Use a different tag if <h1> is not found\n",
    "    if article_title is None:\n",
    "        print(f\"Title not found for URL_ID: {url_id}\")\n",
    "        continue \n",
    "        \n",
    "    article_title = article_title.text.strip()\n",
    "        \n",
    "    article_text = ''\n",
    "    article_content = soup.find('article')\n",
    "    \n",
    "    if article_content is None:\n",
    "        continue\n",
    "    paragraphs = article_content.find_all('p')\n",
    "    for paragraph in paragraphs:\n",
    "        article_text += paragraph.text.strip() + '\\n'\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the file name with URL_ID\n",
    "    file_name = f'{url_id}.txt'\n",
    "    \n",
    "     # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create the file path\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Save the extracted article in a text file\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(article_title + '\\n\\n')\n",
    "        file.write(article_text)\n",
    "\n",
    "    print(f\"Article saved for URL_ID: {url_id}\", file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58262fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "                 \n",
    "print(\"Current working directory:\", current_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a0d84",
   "metadata": {},
   "source": [
    "## Removing stop words from each text file generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497648f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from file: blackassign0001.txt\n",
      "Stopwords removed from file: blackassign0002.txt\n",
      "Stopwords removed from file: blackassign0003.txt\n",
      "Stopwords removed from file: blackassign0004.txt\n",
      "Stopwords removed from file: blackassign0005.txt\n",
      "Stopwords removed from file: blackassign0006.txt\n",
      "Stopwords removed from file: blackassign0007.txt\n",
      "Stopwords removed from file: blackassign0008.txt\n",
      "Stopwords removed from file: blackassign0009.txt\n",
      "Stopwords removed from file: blackassign0010.txt\n",
      "Stopwords removed from file: blackassign0011.txt\n",
      "Stopwords removed from file: blackassign0012.txt\n",
      "Stopwords removed from file: blackassign0013.txt\n",
      "Stopwords removed from file: blackassign0014.txt\n",
      "Stopwords removed from file: blackassign0015.txt\n",
      "Stopwords removed from file: blackassign0016.txt\n",
      "Stopwords removed from file: blackassign0017.txt\n",
      "Stopwords removed from file: blackassign0018.txt\n",
      "Stopwords removed from file: blackassign0019.txt\n",
      "Stopwords removed from file: blackassign0020.txt\n",
      "Stopwords removed from file: blackassign0021.txt\n",
      "Stopwords removed from file: blackassign0022.txt\n",
      "Stopwords removed from file: blackassign0023.txt\n",
      "Stopwords removed from file: blackassign0024.txt\n",
      "Stopwords removed from file: blackassign0025.txt\n",
      "Stopwords removed from file: blackassign0026.txt\n",
      "Stopwords removed from file: blackassign0027.txt\n",
      "Stopwords removed from file: blackassign0028.txt\n",
      "Stopwords removed from file: blackassign0029.txt\n",
      "Stopwords removed from file: blackassign0030.txt\n",
      "Stopwords removed from file: blackassign0031.txt\n",
      "Stopwords removed from file: blackassign0032.txt\n",
      "Stopwords removed from file: blackassign0033.txt\n",
      "Stopwords removed from file: blackassign0034.txt\n",
      "Stopwords removed from file: blackassign0035.txt\n",
      "Stopwords removed from file: blackassign0037.txt\n",
      "Stopwords removed from file: blackassign0038.txt\n",
      "Stopwords removed from file: blackassign0039.txt\n",
      "Stopwords removed from file: blackassign0040.txt\n",
      "Stopwords removed from file: blackassign0041.txt\n",
      "Stopwords removed from file: blackassign0042.txt\n",
      "Stopwords removed from file: blackassign0043.txt\n",
      "Stopwords removed from file: blackassign0044.txt\n",
      "Stopwords removed from file: blackassign0045.txt\n",
      "Stopwords removed from file: blackassign0046.txt\n",
      "Stopwords removed from file: blackassign0047.txt\n",
      "Stopwords removed from file: blackassign0048.txt\n",
      "Stopwords removed from file: blackassign0050.txt\n",
      "Stopwords removed from file: blackassign0051.txt\n",
      "Stopwords removed from file: blackassign0052.txt\n",
      "Stopwords removed from file: blackassign0053.txt\n",
      "Stopwords removed from file: blackassign0054.txt\n",
      "Stopwords removed from file: blackassign0055.txt\n",
      "Stopwords removed from file: blackassign0056.txt\n",
      "Stopwords removed from file: blackassign0057.txt\n",
      "Stopwords removed from file: blackassign0058.txt\n",
      "Stopwords removed from file: blackassign0059.txt\n",
      "Stopwords removed from file: blackassign0060.txt\n",
      "Stopwords removed from file: blackassign0061.txt\n",
      "Stopwords removed from file: blackassign0062.txt\n",
      "Stopwords removed from file: blackassign0063.txt\n",
      "Stopwords removed from file: blackassign0064.txt\n",
      "Stopwords removed from file: blackassign0065.txt\n",
      "Stopwords removed from file: blackassign0066.txt\n",
      "Stopwords removed from file: blackassign0067.txt\n",
      "Stopwords removed from file: blackassign0068.txt\n",
      "Stopwords removed from file: blackassign0069.txt\n",
      "Stopwords removed from file: blackassign0070.txt\n",
      "Stopwords removed from file: blackassign0071.txt\n",
      "Stopwords removed from file: blackassign0072.txt\n",
      "Stopwords removed from file: blackassign0073.txt\n",
      "Stopwords removed from file: blackassign0074.txt\n",
      "Stopwords removed from file: blackassign0075.txt\n",
      "Stopwords removed from file: blackassign0076.txt\n",
      "Stopwords removed from file: blackassign0077.txt\n",
      "Stopwords removed from file: blackassign0078.txt\n",
      "Stopwords removed from file: blackassign0079.txt\n",
      "Stopwords removed from file: blackassign0080.txt\n",
      "Stopwords removed from file: blackassign0081.txt\n",
      "Stopwords removed from file: blackassign0082.txt\n",
      "Stopwords removed from file: blackassign0083.txt\n",
      "Stopwords removed from file: blackassign0084.txt\n",
      "Stopwords removed from file: blackassign0085.txt\n",
      "Stopwords removed from file: blackassign0086.txt\n",
      "Stopwords removed from file: blackassign0087.txt\n",
      "Stopwords removed from file: blackassign0088.txt\n",
      "Stopwords removed from file: blackassign0089.txt\n",
      "Stopwords removed from file: blackassign0090.txt\n",
      "Stopwords removed from file: blackassign0091.txt\n",
      "Stopwords removed from file: blackassign0092.txt\n",
      "Stopwords removed from file: blackassign0093.txt\n",
      "Stopwords removed from file: blackassign0094.txt\n",
      "Stopwords removed from file: blackassign0095.txt\n",
      "Stopwords removed from file: blackassign0096.txt\n",
      "Stopwords removed from file: blackassign0097.txt\n",
      "Stopwords removed from file: blackassign0098.txt\n",
      "Stopwords removed from file: blackassign0099.txt\n",
      "Stopwords removed from file: blackassign0100.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# List of encodings to try\n",
    "encodings = ['utf-8-sig', 'latin-1', 'utf-16', 'cp1252', 'iso-8859-2', 'windows-1252']\n",
    "\n",
    "# Set the directory containing the stopword text files\n",
    "stopwords_directory = r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\StopWords'\n",
    "\n",
    "\n",
    "# Get the list of stopwords text files in the directory\n",
    "stopwords_files = os.listdir(stopwords_directory)\n",
    "\n",
    "# Create an empty set to store all stopwords\n",
    "stopwords_set = set()\n",
    "\n",
    "# Iterate over each stopwords text file\n",
    "for file_name in stopwords_files:\n",
    "    file_path = os.path.join(stopwords_directory, file_name)\n",
    "\n",
    "    # Read the stopwords from the file using different encodings\n",
    "    content = None\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.read()\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    # If no encoding succeeds, skip the file\n",
    "    if content is None:\n",
    "        print(f\"Unable to read file: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    stopwords_set.update(content.splitlines())\n",
    "\n",
    "# Set the directory containing the text files\n",
    "text_files_directory = r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results'\n",
    "\n",
    "\n",
    "# Get the list of text files in the directory\n",
    "text_files = os.listdir(text_files_directory)\n",
    "\n",
    "# Iterate over each text file\n",
    "for file_name in text_files:\n",
    "    file_path = os.path.join(text_files_directory, file_name)\n",
    "\n",
    "    # Read the content of the text file using different encodings\n",
    "    content = None\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.read()\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    # If no encoding succeeds, skip the file\n",
    "    if content is None:\n",
    "        print(f\"Unable to read file: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    # Tokenize the content into words\n",
    "    words = content.split()\n",
    "\n",
    "    # Remove stopwords from the tokenized words\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords_set]\n",
    "\n",
    "    # Join the remaining words back into a string\n",
    "    filtered_content = ' '.join(filtered_words)\n",
    "\n",
    "    # Overwrite the text file with the updated content\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as file:\n",
    "        file.write(filtered_content)\n",
    "\n",
    "    print(f\"Stopwords removed from file: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ef577",
   "metadata": {},
   "source": [
    "## Calculate positive / negative and polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c032fc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackassign0001.txt  positive score :  6  negative score :  1  Polarity_Score :  0.7142856122449125\n",
      "Subjectivity_Score :  0.039999999771428575\n",
      "blackassign0002.txt  positive score :  45  negative score :  23  Polarity_Score :  0.3235294070069205\n",
      "Subjectivity_Score :  0.0878552970441146\n",
      "blackassign0003.txt  positive score :  33  negative score :  19  Polarity_Score :  0.2692307640532545\n",
      "Subjectivity_Score :  0.08253968240866717\n",
      "blackassign0004.txt  positive score :  29  negative score :  58  Polarity_Score :  -0.33333332950191574\n",
      "Subjectivity_Score :  0.14262295058586402\n",
      "blackassign0005.txt  positive score :  17  negative score :  7  Polarity_Score :  0.41666664930555625\n",
      "Subjectivity_Score :  0.06451612885882761\n",
      "blackassign0006.txt  positive score :  70  negative score :  21  Polarity_Score :  0.5384615325443788\n",
      "Subjectivity_Score :  0.08333333325702075\n",
      "blackassign0007.txt  positive score :  21  negative score :  28  Polarity_Score :  -0.14285713994169102\n",
      "Subjectivity_Score :  0.0989898987899194\n",
      "blackassign0008.txt  positive score :  27  negative score :  7  Polarity_Score :  0.5882352768166096\n",
      "Subjectivity_Score :  0.06967213100477022\n",
      "blackassign0009.txt  positive score :  27  negative score :  36  Polarity_Score :  -0.1428571405895692\n",
      "Subjectivity_Score :  0.10482529100694628\n",
      "blackassign0010.txt  positive score :  53  negative score :  51  Polarity_Score :  0.01923076904585799\n",
      "Subjectivity_Score :  0.1414965984469434\n",
      "blackassign0011.txt  positive score :  53  negative score :  13  Polarity_Score :  0.6060605968778697\n",
      "Subjectivity_Score :  0.08354430369171607\n",
      "blackassign0012.txt  positive score :  65  negative score :  19  Polarity_Score :  0.5476190410997733\n",
      "Subjectivity_Score :  0.0972222221096965\n",
      "blackassign0013.txt  positive score :  37  negative score :  11  Polarity_Score :  0.5416666553819447\n",
      "Subjectivity_Score :  0.16724738617683838\n",
      "blackassign0014.txt  positive score :  18  negative score :  20  Polarity_Score :  -0.05263157756232691\n",
      "Subjectivity_Score :  0.06871609390828916\n",
      "blackassign0015.txt  positive score :  31  negative score :  24  Polarity_Score :  0.12727272495867772\n",
      "Subjectivity_Score :  0.08245877049106631\n",
      "blackassign0016.txt  positive score :  31  negative score :  24  Polarity_Score :  0.12727272495867772\n",
      "Subjectivity_Score :  0.08245877049106631\n",
      "blackassign0017.txt  positive score :  36  negative score :  10  Polarity_Score :  0.5652173790170135\n",
      "Subjectivity_Score :  0.07809847185382263\n",
      "blackassign0018.txt  positive score :  28  negative score :  6  Polarity_Score :  0.6470588044982705\n",
      "Subjectivity_Score :  0.05841924388587759\n",
      "blackassign0019.txt  positive score :  45  negative score :  5  Polarity_Score :  0.7999999840000004\n",
      "Subjectivity_Score :  0.05854800929912411\n",
      "blackassign0020.txt  positive score :  7  negative score :  0  Polarity_Score :  0.9999998571428775\n",
      "Subjectivity_Score :  0.03271028022098\n",
      "blackassign0021.txt  positive score :  18  negative score :  34  Polarity_Score :  -0.30769230177514806\n",
      "Subjectivity_Score :  0.09541284386162781\n",
      "blackassign0022.txt  positive score :  11  negative score :  8  Polarity_Score :  0.15789472853185638\n",
      "Subjectivity_Score :  0.10270270214755296\n",
      "blackassign0023.txt  positive score :  29  negative score :  10  Polarity_Score :  0.4871794746877058\n",
      "Subjectivity_Score :  0.056851311870479135\n",
      "blackassign0024.txt  positive score :  20  negative score :  2  Polarity_Score :  0.8181817809917372\n",
      "Subjectivity_Score :  0.07236842081457756\n",
      "blackassign0025.txt  positive score :  17  negative score :  9  Polarity_Score :  0.30769229585798863\n",
      "Subjectivity_Score :  0.04952380942947846\n",
      "blackassign0026.txt  positive score :  20  negative score :  13  Polarity_Score :  0.21212120569329682\n",
      "Subjectivity_Score :  0.06707317059537973\n",
      "blackassign0027.txt  positive score :  21  negative score :  19  Polarity_Score :  0.04999999875000003\n",
      "Subjectivity_Score :  0.06633499159811776\n",
      "blackassign0028.txt  positive score :  26  negative score :  23  Polarity_Score :  0.06122448854643901\n",
      "Subjectivity_Score :  0.08566433551457284\n",
      "blackassign0029.txt  positive score :  55  negative score :  26  Polarity_Score :  0.35802468693796685\n",
      "Subjectivity_Score :  0.07803468200574692\n",
      "blackassign0030.txt  positive score :  45  negative score :  28  Polarity_Score :  0.23287670913867523\n",
      "Subjectivity_Score :  0.11060606043847566\n",
      "blackassign0031.txt  positive score :  57  negative score :  29  Polarity_Score :  0.3255813915630071\n",
      "Subjectivity_Score :  0.09695603145777223\n",
      "blackassign0032.txt  positive score :  39  negative score :  14  Polarity_Score :  0.47169810430758297\n",
      "Subjectivity_Score :  0.07220708437029008\n",
      "blackassign0033.txt  positive score :  42  negative score :  18  Polarity_Score :  0.39999999333333347\n",
      "Subjectivity_Score :  0.07050528781374232\n",
      "blackassign0034.txt  positive score :  31  negative score :  16  Polarity_Score :  0.31914892937981004\n",
      "Subjectivity_Score :  0.0784641067137494\n",
      "blackassign0035.txt  positive score :  19  negative score :  9  Polarity_Score :  0.35714284438775556\n",
      "Subjectivity_Score :  0.0760869563149811\n",
      "blackassign0037.txt  positive score :  29  negative score :  9  Polarity_Score :  0.5263157756232691\n",
      "Subjectivity_Score :  0.10951008613974039\n",
      "blackassign0038.txt  positive score :  55  negative score :  31  Polarity_Score :  0.2790697641968632\n",
      "Subjectivity_Score :  0.08293153318907276\n",
      "blackassign0039.txt  positive score :  38  negative score :  40  Polarity_Score :  -0.02564102531229455\n",
      "Subjectivity_Score :  0.07902735554303206\n",
      "blackassign0040.txt  positive score :  30  negative score :  15  Polarity_Score :  0.3333333259259261\n",
      "Subjectivity_Score :  0.07281553386275803\n",
      "blackassign0041.txt  positive score :  21  negative score :  20  Polarity_Score :  0.024390243307555043\n",
      "Subjectivity_Score :  0.06336939711998547\n",
      "blackassign0042.txt  positive score :  54  negative score :  23  Polarity_Score :  0.40259739736886496\n",
      "Subjectivity_Score :  0.10999999984285715\n",
      "blackassign0043.txt  positive score :  46  negative score :  17  Polarity_Score :  0.4603174530108341\n",
      "Subjectivity_Score :  0.07944514491873247\n",
      "blackassign0044.txt  positive score :  18  negative score :  0  Polarity_Score :  0.9999999444444475\n",
      "Subjectivity_Score :  0.06666666641975309\n",
      "blackassign0045.txt  positive score :  78  negative score :  37  Polarity_Score :  0.3565217360302458\n",
      "Subjectivity_Score :  0.14877102179977875\n",
      "blackassign0046.txt  positive score :  17  negative score :  0  Polarity_Score :  0.999999941176474\n",
      "Subjectivity_Score :  0.044973544854567345\n",
      "blackassign0047.txt  positive score :  11  negative score :  6  Polarity_Score :  0.2941176297577865\n",
      "Subjectivity_Score :  0.045576407384513656\n",
      "blackassign0048.txt  positive score :  0  negative score :  3  Polarity_Score :  -0.9999996666667778\n",
      "Subjectivity_Score :  0.0348837205246079\n",
      "blackassign0050.txt  positive score :  23  negative score :  60  Polarity_Score :  -0.44578312715923946\n",
      "Subjectivity_Score :  0.1180654336869624\n",
      "blackassign0051.txt  positive score :  15  negative score :  9  Polarity_Score :  0.24999998958333375\n",
      "Subjectivity_Score :  0.06741573014770863\n",
      "blackassign0052.txt  positive score :  48  negative score :  24  Polarity_Score :  0.3333333287037038\n",
      "Subjectivity_Score :  0.0849056602772339\n",
      "blackassign0053.txt  positive score :  33  negative score :  1  Polarity_Score :  0.9411764429065753\n",
      "Subjectivity_Score :  0.10303030271809\n",
      "blackassign0054.txt  positive score :  3  negative score :  1  Polarity_Score :  0.49999987500003124\n",
      "Subjectivity_Score :  0.0202020200999898\n",
      "blackassign0055.txt  positive score :  18  negative score :  5  Polarity_Score :  0.5652173667296797\n",
      "Subjectivity_Score :  0.057213930205935494\n",
      "blackassign0056.txt  positive score :  11  negative score :  8  Polarity_Score :  0.15789472853185638\n",
      "Subjectivity_Score :  0.07251908369267525\n",
      "blackassign0057.txt  positive score :  13  negative score :  6  Polarity_Score :  0.36842103324099823\n",
      "Subjectivity_Score :  0.11656441646279499\n",
      "blackassign0058.txt  positive score :  5  negative score :  0  Polarity_Score :  0.99999980000004\n",
      "Subjectivity_Score :  0.05813953420767984\n",
      "blackassign0059.txt  positive score :  17  negative score :  21  Polarity_Score :  -0.10526315512465383\n",
      "Subjectivity_Score :  0.10187667533008934\n",
      "blackassign0060.txt  positive score :  7  negative score :  2  Polarity_Score :  0.5555554938271674\n",
      "Subjectivity_Score :  0.0999999988888889\n",
      "blackassign0061.txt  positive score :  32  negative score :  7  Polarity_Score :  0.6410256245890866\n",
      "Subjectivity_Score :  0.10427807458749178\n",
      "blackassign0062.txt  positive score :  1  negative score :  5  Polarity_Score :  -0.6666665555555741\n",
      "Subjectivity_Score :  0.08955223746936959\n",
      "blackassign0063.txt  positive score :  8  negative score :  21  Polarity_Score :  -0.4482758466111777\n",
      "Subjectivity_Score :  0.18831168708888515\n",
      "blackassign0064.txt  positive score :  24  negative score :  37  Polarity_Score :  -0.21311475060467622\n",
      "Subjectivity_Score :  0.06900452480881841\n",
      "blackassign0065.txt  positive score :  34  negative score :  22  Polarity_Score :  0.21428571045918376\n",
      "Subjectivity_Score :  0.07244501931119661\n",
      "blackassign0066.txt  positive score :  26  negative score :  19  Polarity_Score :  0.1555555520987655\n",
      "Subjectivity_Score :  0.06181318672827859\n",
      "blackassign0067.txt  positive score :  19  negative score :  2  Polarity_Score :  0.8095237709750585\n",
      "Subjectivity_Score :  0.06017191959836126\n",
      "blackassign0068.txt  positive score :  34  negative score :  5  Polarity_Score :  0.7435897245233405\n",
      "Subjectivity_Score :  0.07129798890073494\n",
      "blackassign0069.txt  positive score :  18  negative score :  6  Polarity_Score :  0.4999999791666675\n",
      "Subjectivity_Score :  0.12499999934895833\n",
      "blackassign0070.txt  positive score :  35  negative score :  9  Polarity_Score :  0.5909090774793392\n",
      "Subjectivity_Score :  0.052631578884411986\n",
      "blackassign0071.txt  positive score :  16  negative score :  54  Polarity_Score :  -0.5428571351020409\n",
      "Subjectivity_Score :  0.16091953985995508\n",
      "blackassign0072.txt  positive score :  45  negative score :  14  Polarity_Score :  0.5254237199080726\n",
      "Subjectivity_Score :  0.11216730016698231\n",
      "blackassign0073.txt  positive score :  17  negative score :  23  Polarity_Score :  -0.1499999962500001\n",
      "Subjectivity_Score :  0.07532956671312699\n",
      "blackassign0074.txt  positive score :  21  negative score :  16  Polarity_Score :  0.1351351314828343\n",
      "Subjectivity_Score :  0.06764168177762032\n",
      "blackassign0075.txt  positive score :  14  negative score :  60  Polarity_Score :  -0.6216216132213296\n",
      "Subjectivity_Score :  0.16444444407901235\n",
      "blackassign0076.txt  positive score :  16  negative score :  6  Polarity_Score :  0.4545454338842984\n",
      "Subjectivity_Score :  0.0672782872560297\n",
      "blackassign0077.txt  positive score :  5  negative score :  3  Polarity_Score :  0.24999996875000394\n",
      "Subjectivity_Score :  0.055944055552838774\n",
      "blackassign0078.txt  positive score :  22  negative score :  13  Polarity_Score :  0.2571428497959186\n",
      "Subjectivity_Score :  0.08928571405794461\n",
      "blackassign0079.txt  positive score :  16  negative score :  24  Polarity_Score :  -0.19999999500000012\n",
      "Subjectivity_Score :  0.06472491898911825\n",
      "blackassign0080.txt  positive score :  16  negative score :  10  Polarity_Score :  0.23076922189349144\n",
      "Subjectivity_Score :  0.01168014375036831\n",
      "blackassign0081.txt  positive score :  30  negative score :  34  Polarity_Score :  -0.062499999023437516\n",
      "Subjectivity_Score :  0.07710843364203804\n",
      "blackassign0082.txt  positive score :  25  negative score :  46  Polarity_Score :  -0.2957746437214839\n",
      "Subjectivity_Score :  0.08626974473114248\n",
      "blackassign0083.txt  positive score :  2  negative score :  1  Polarity_Score :  0.33333322222225925\n",
      "Subjectivity_Score :  0.04615384544378699\n",
      "blackassign0084.txt  positive score :  26  negative score :  4  Polarity_Score :  0.7333333088888897\n",
      "Subjectivity_Score :  0.07352941158448674\n",
      "blackassign0085.txt  positive score :  25  negative score :  30  Polarity_Score :  -0.09090908925619838\n",
      "Subjectivity_Score :  0.07343124155750168\n",
      "blackassign0086.txt  positive score :  38  negative score :  14  Polarity_Score :  0.4615384526627221\n",
      "Subjectivity_Score :  0.06220095686339598\n",
      "blackassign0087.txt  positive score :  21  negative score :  26  Polarity_Score :  -0.10638297645993668\n",
      "Subjectivity_Score :  0.08671586699867921\n",
      "blackassign0088.txt  positive score :  14  negative score :  38  Polarity_Score :  -0.4615384526627221\n",
      "Subjectivity_Score :  0.054507337469069876\n",
      "blackassign0089.txt  positive score :  12  negative score :  36  Polarity_Score :  -0.4999999895833336\n",
      "Subjectivity_Score :  0.09876543189554438\n",
      "blackassign0090.txt  positive score :  30  negative score :  35  Polarity_Score :  -0.07692307573964499\n",
      "Subjectivity_Score :  0.1188299815012249\n",
      "blackassign0091.txt  positive score :  20  negative score :  23  Polarity_Score :  -0.06976744023796651\n",
      "Subjectivity_Score :  0.08634538135272012\n",
      "blackassign0092.txt  positive score :  16  negative score :  28  Polarity_Score :  -0.27272726652892576\n",
      "Subjectivity_Score :  0.062234794820035653\n",
      "blackassign0093.txt  positive score :  3  negative score :  2  Polarity_Score :  0.199999960000008\n",
      "Subjectivity_Score :  0.05952380881519275\n",
      "blackassign0094.txt  positive score :  27  negative score :  39  Polarity_Score :  -0.18181817906336092\n",
      "Subjectivity_Score :  0.1155866898150846\n",
      "blackassign0095.txt  positive score :  6  negative score :  22  Polarity_Score :  -0.5714285510204089\n",
      "Subjectivity_Score :  0.07977207954480889\n",
      "blackassign0096.txt  positive score :  26  negative score :  48  Polarity_Score :  -0.29729729327976634\n",
      "Subjectivity_Score :  0.12671232855015013\n",
      "blackassign0097.txt  positive score :  21  negative score :  31  Polarity_Score :  -0.19230768860946754\n",
      "Subjectivity_Score :  0.1140350874692213\n",
      "blackassign0098.txt  positive score :  0  negative score :  0  Polarity_Score :  0.0\n",
      "Subjectivity_Score :  0.0\n",
      "blackassign0099.txt  positive score :  10  negative score :  2  Polarity_Score :  0.6666666111111158\n",
      "Subjectivity_Score :  0.043956043795032804\n",
      "blackassign0100.txt  positive score :  26  negative score :  41  Polarity_Score :  -0.22388059367342397\n",
      "Subjectivity_Score :  0.14439655141293847\n"
     ]
    }
   ],
   "source": [
    "#positive negative words count\n",
    "text_files_directory = r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results'\n",
    "\n",
    "text_files = os.listdir(text_files_directory)\n",
    "\n",
    "master_directory = r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\MasterDictionary'\n",
    "\n",
    "master_dic_files = os.listdir(master_directory)\n",
    "\n",
    "positive_words=[]\n",
    "negative_words=[]\n",
    "\n",
    "for file_name in master_dic_files :\n",
    "    file_path = os.path.join(master_directory, file_name)\n",
    "    \n",
    "    content = None\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.read()\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    # If no encoding succeeds, skip the file\n",
    "    if content is None:\n",
    "        print(f\"Unable to read file: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        content = file.read()\n",
    "    #tokenize into words\n",
    "    words = content.split()\n",
    "#     print(file_name)\n",
    "        \n",
    "    if file_name == 'negative-words.txt' :\n",
    "        for word in words:\n",
    "            negative_words.append(word)\n",
    "    else:\n",
    "        for word in words:\n",
    "            positive_words.append(word)\n",
    "# print(negative_words)\n",
    "\n",
    "\n",
    "# Polarity_Score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "\n",
    "for file_name in text_files:\n",
    "    positive_score=negative_score=0\n",
    "    file_path = os.path.join(text_files_directory, file_name)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    # Tokenize the content into words\n",
    "    words = content.split()\n",
    "    \n",
    "    \n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            positive_score+=1\n",
    "            \n",
    "        elif word in negative_words:\n",
    "            negative_score+=1\n",
    "            \n",
    "    Polarity_Score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    Subjectivity_Score = (positive_score + negative_score)/ ((len(words)) + 0.000001)\n",
    "    \n",
    "    print(file_name,\" positive score : \",positive_score,\" negative score : \",negative_score,\" Polarity_Score : \",Polarity_Score)\n",
    "            \n",
    "    print(\"Subjectivity_Score : \",Subjectivity_Score)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a84a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e480120",
   "metadata": {},
   "source": [
    "## The required function to calculate values of the remaining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a2139aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_personal_pronouns(file_path):\n",
    "    # Read the content of the text file using different encodings\n",
    "    content = None\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.read()\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # If no encoding succeeds, return 0\n",
    "    if content is None:\n",
    "        print(f\"Unable to read file: {file_path}\")\n",
    "        return 0\n",
    "\n",
    "    # Define the pattern for personal pronouns\n",
    "    pattern = r\"\\b(?!(?:US|us)\\b)(?:I|we|my|ours|us)\\b\"\n",
    "\n",
    "    # Find all matches of the pattern in the content\n",
    "    matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "\n",
    "    # Return the count of personal pronouns\n",
    "    return len(matches)\n",
    "\n",
    "text_files_directory = r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results'\n",
    "\n",
    "# Get the list of text files in the directory\n",
    "text_files = os.listdir(text_files_directory)\n",
    "\n",
    "\n",
    "def tokenize_character_wise(text):\n",
    "    # Split the text into individual characters\n",
    "    tokens = list(text)\n",
    "\n",
    "    # Return the list of character tokens\n",
    "    return tokens\n",
    "\n",
    "#function to calculate the number of syllabels\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiouy'\n",
    "    count = 0\n",
    "    previous_was_vowel = False\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            if not previous_was_vowel:\n",
    "                count += 1\n",
    "            previous_was_vowel = True\n",
    "        else:\n",
    "            previous_was_vowel = False\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    return count\n",
    "\n",
    "#function to get the filename for matching the data with excel sheet URL_ID\n",
    "def extract_url_id(file_name):\n",
    "    return file_name.split('.')[0]  # Assuming the file name is in the format URL_ID.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0126c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'URL_ID': 'A', 'URL': 'B', 'POSITIVE SCORE': 'C', 'NEGATIVE SCORE': 'D', 'POLARITY SCORE': 'E', 'SUBJECTIVITY SCORE': 'F', 'AVG SENTENCE LENGTH': 'G', 'PERCENTAGE OF COMPLEX WORDS': 'H', 'FOG INDEX': 'I', 'AVG NUMBER OF WORDS PER SENTENCE': 'J', 'COMPLEX WORD COUNT': 'K', 'WORD COUNT': 'L', 'SYLLABLE PER WORD': 'M', 'PERSONAL PRONOUNS': 'N', 'AVG WORD LENGTH': 'O', None: 'Z'}\n"
     ]
    }
   ],
   "source": [
    "print(column_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eb284db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import openpyxl\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Set the directory containing the text files\n",
    "text_files_directory = r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\results'\n",
    "\n",
    "# Path to the output Excel file\n",
    "output_file = r'C:\\Users\\Abhi\\Desktop\\20211030 Test Assignment\\Output Data Structure.xlsx'\n",
    "\n",
    "# Get the list of text files in the directory\n",
    "text_files = os.listdir(text_files_directory)\n",
    "\n",
    "# Translator for removing punctuation\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Load the output Excel file\n",
    "workbook = openpyxl.load_workbook(output_file)\n",
    "\n",
    "# Select the active sheet\n",
    "sheet = workbook.active\n",
    "\n",
    "# Find the column indexes based on the column names\n",
    "column_names = sheet[1]\n",
    "column_indexes = {column.value: column.column_letter for column in column_names}\n",
    "\n",
    "# Counter for the current row\n",
    "row_counter = 2\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each text file\n",
    "for file_name in text_files:\n",
    "    complex_words = 0\n",
    "    file_path = os.path.join(text_files_directory, file_name)\n",
    "    \n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "     # Tokenize the content into words\n",
    "    words = content.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            positive_score+=1\n",
    "            \n",
    "        elif word in negative_words:\n",
    "            negative_score+=1\n",
    "            \n",
    "    Polarity_Score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    Subjectivity_Score = (positive_score + negative_score)/ ((len(words)) + 0.000001)\n",
    "    \n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "\n",
    "    # Print the word and its corresponding syllable count\n",
    "    for word, syllable_count in zip(words, syllable_counts):\n",
    "        if syllable_count>2 :\n",
    "            complex_words+=1\n",
    "\n",
    "    \n",
    "    per_compl_words= complex_words / (len(words))\n",
    "    \n",
    "    Average_Sentence_Length = len(words) / sentence_count\n",
    "    \n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + per_compl_words)\n",
    "    \n",
    "    Average_Number_of_Words_Per_Sentence = len(words) / sentence_count\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Count the number of syllables in each word\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "#     print(syllable_counts)\n",
    "    syllable_per_word= sum(syllable_counts)/len(words)\n",
    "    \n",
    "    # Print the word and its corresponding syllable count\n",
    "    for word, syllable_count in zip(words, syllable_counts):\n",
    "        if syllable_count > 2:\n",
    "            complex_words += 1\n",
    "\n",
    "    complex_word_percentage = complex_words / len(words)\n",
    "    word_count = len(words)\n",
    "    \n",
    "    \n",
    "    tokens = tokenize_character_wise(content)\n",
    "        \n",
    "    char_len=len(tokens)\n",
    "    \n",
    "    Avg_word_len=char_len/len(words)\n",
    "    \n",
    "    pronoun_count = count_personal_pronouns(file_path)\n",
    "    \n",
    "    url_id = extract_url_id (file_name)\n",
    "    \n",
    "    if (sheet[column_indexes['URL_ID'] + str(row_counter)] .value == url_id ):\n",
    "            row_counter = row_counter\n",
    "    else:\n",
    "        \n",
    "        row_counter+=1\n",
    "    \n",
    "    # Write the values to the corresponding cells in the sheet\n",
    "    sheet[column_indexes['POSITIVE SCORE'] + str(row_counter)] = positive_score\n",
    "    sheet[column_indexes['NEGATIVE SCORE'] + str(row_counter)] = negative_score\n",
    "    sheet[column_indexes['POLARITY SCORE'] + str(row_counter)] = Polarity_Score\n",
    "    sheet[column_indexes['SUBJECTIVITY SCORE'] + str(row_counter)] = Subjectivity_Score\n",
    "    sheet[column_indexes['AVG SENTENCE LENGTH'] + str(row_counter)] = Average_Sentence_Length\n",
    "    sheet[column_indexes['PERCENTAGE OF COMPLEX WORDS'] + str(row_counter)] = complex_word_percentage\n",
    "    sheet[column_indexes['FOG INDEX'] + str(row_counter)] = Fog_Index\n",
    "    sheet[column_indexes['AVG NUMBER OF WORDS PER SENTENCE'] + str(row_counter)] = Average_Sentence_Length\n",
    "    sheet[column_indexes['COMPLEX WORD COUNT'] + str(row_counter)] = complex_words\n",
    "    \n",
    "    sheet[column_indexes['SYLLABLE PER WORD'] + str(row_counter)] = syllable_per_word\n",
    "    \n",
    "    sheet[column_indexes['PERSONAL PRONOUNS'] + str(row_counter)] = pronoun_count\n",
    "    sheet[column_indexes['AVG WORD LENGTH'] + str(row_counter)] = Avg_word_len\n",
    "    # Increment the row counter\n",
    "    row_counter += 1\n",
    "\n",
    "# Save the updated Excel file\n",
    "workbook.save(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5adda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d53283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831b38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
